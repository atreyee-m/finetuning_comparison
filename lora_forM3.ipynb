{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning for Text Classification on Apple M3\n",
    "\n",
    "This notebook implements LoRA (Low-Rank Adaptation) fine-tuning for text classification using the AG News dataset, optimized specifically for Apple M3 chip\n",
    "I'm having some issues with the huggingface trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (4.48.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: peft in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (1.3.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (2.5.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/documentqa/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-macosx_10_13_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.1 seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets evaluate scikit-learn peft accelerate torch matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/documentqa/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up Device\n",
    "\n",
    "Check if MPS (Metal Performance Shaders) is available on this Apple Silicon M3 device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "PyTorch version: 2.5.1\n",
      "MPS device is available and will be used for training\n"
     ]
    }
   ],
   "source": [
    "# Check for MPS availability on Apple Silicon\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For M3 chips, verify the PyTorch version supports all M3 features\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == \"mps\":\n",
    "    print(\"MPS device is available and will be used for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "We'll use the AG News dataset, which contains news articles classified into 4 categories: World, Sports, Business, and Sci/Tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 120000 training examples and 7600 test examples\n",
      "\n",
      "Sample training example:\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "\n",
      "Class distribution in training set:\n",
      "Class 2: 30000 examples (25.00%)\n",
      "Class 3: 30000 examples (25.00%)\n",
      "Class 1: 30000 examples (25.00%)\n",
      "Class 0: 30000 examples (25.00%)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "print(f\"Dataset loaded with {len(dataset['train'])} training examples and {len(dataset['test'])} test examples\")\n",
    "\n",
    "# Examine a sample\n",
    "print(\"\\nSample training example:\")\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# Check class distribution\n",
    "label_counts = {}\n",
    "for example in dataset['train']:\n",
    "    label = example['label']\n",
    "    label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Class {label}: {count} examples ({count/len(dataset['train'])*100:.2f}%)\")\n",
    "\n",
    "# Create class label mapping\n",
    "label_mapping = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Up Tokenizer\n",
    "\n",
    "M3 should be fine with larger LLMs too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"distilbert-base-uncased\"  # Smaller, faster model\n",
    "model_name = \"bert-base-uncased\"        \n",
    "# model_name = \"roberta-base\"           \n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120000/120000 [00:04<00:00, 24781.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing testing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7600/7600 [00:00<00:00, 19069.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "print(\"Tokenizing training dataset...\")\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "print(\"Tokenizing testing dataset...\")\n",
    "tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Format datasets for PyTorch\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create DataLoaders with Batch Size Optimized for M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoaders with batch size 16\n",
      "Training batches: 7500\n",
      "Evaluation batches: 475\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders with batch size optimized for M3\n",
    "batch_size = 16  # M3 can handle larger batches than M1/M2\n",
    "train_dataloader = DataLoader(tokenized_train, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_test, batch_size=batch_size)\n",
    "\n",
    "print(f\"Created DataLoaders with batch size {batch_size}\")\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Evaluation batches: {len(eval_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Model and Apply LoRA\n",
    "\n",
    "LoRA works by adding small \"adapter\" layers to the model, allowing us to fine-tune efficiently by updating only a small fraction of the parameters. We can use a higher rank value  for M3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/documentqa/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 1,920,004 || all params: 111,405,320 || trainable%: 1.7234\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {model_name}...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "if \"distilbert\" in model_name:\n",
    "    target_modules = [\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]\n",
    "elif \"bert\" in model_name:\n",
    "    target_modules = [\"query\", \"key\", \"value\", \"output.dense\"]\n",
    "elif \"roberta\" in model_name:\n",
    "    target_modules = [\"query\", \"key\", \"value\", \"output.dense\"]\n",
    "else:\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,                           # Higher rank for M3 (more expressive adapters)\n",
    "    lora_alpha=32,                  # Scaling factor\n",
    "    target_modules=target_modules,  # Model-specific layers to adapt\n",
    "    lora_dropout=0.1,               # Dropout probability for LoRA layers\n",
    "    bias=\"none\",                    # Don't train bias parameters\n",
    "    task_type=TaskType.SEQ_CLS      # Sequence classification task\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, config)\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Set Up Optimizer and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer - higher learning rate works well with LoRA\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)  # Slightly higher learning rate for M3\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "num_epochs = 4  # M3 can handle more epochs efficiently\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=2e-3, \n",
    "    total_steps=num_training_steps,\n",
    "    pct_start=0.1  # Warm up for first 10% of training\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Function with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, lr_scheduler, epoch, num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, batch[\"label\"])\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation Function with Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs.logits, batch[\"label\"])\n",
    "            all_losses.append(loss.item())\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch[\"label\"].cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"eval_loss\": np.mean(all_losses)\n",
    "    }\n",
    "    \n",
    "    return all_preds, all_labels, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Loop with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  72%|███████▏  | 5427/7500 [1:07:01<21:36,  1.60it/s, loss=1.39]   "
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "best_accuracy = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Track metrics for plotting\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    avg_loss = train_epoch(model, train_dataloader, optimizer, lr_scheduler, epoch, num_epochs)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation phase\n",
    "    predictions, labels, metrics = evaluate(model, eval_dataloader)\n",
    "    accuracy = metrics[\"accuracy\"]\n",
    "    eval_loss = metrics[\"eval_loss\"]\n",
    "    \n",
    "    eval_losses.append(eval_loss)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Eval Loss: {eval_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"New best model with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Load best model if we saved one\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model state\")\n",
    "\n",
    "# Plot training metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(eval_losses, label='Evaluation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label='Accuracy', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Progression')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation with the best model\n",
    "predictions, labels, _ = evaluate(model, eval_dataloader)\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Get per-class metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=None)\n",
    "\n",
    "# Create a summary table\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': [label_mapping[i] for i in range(4)],\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "print(\"Per-class Performance:\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = [label_mapping[i] for i in range(4)]\n",
    "print(classification_report(labels, predictions, target_names=target_names))\n",
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "peft_model_id = f\"m3-peft-{model_name.split('/')[-1]}-agnews\"\n",
    "model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "print(f\"Model saved to {peft_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference function\n",
    "def predict_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    # Get probability distribution\n",
    "    probs = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    return predicted_class, label_mapping[predicted_class], probs\n",
    "\n",
    "# Test with some examples\n",
    "test_texts = [\n",
    "    \"NASA successfully launches new Mars rover with advanced sampling technology\",\n",
    "    \"Liverpool FC wins dramatic match against Manchester United with last-minute goal\",\n",
    "    \"Stock markets plunge amid concerns about inflation and interest rates\",\n",
    "    \"New research shows promising results for quantum computing breakthroughs\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    _, prediction, probs = predict_text(text)\n",
    "    print(f\"Text: {text}\\nPredicted category: {prediction}\")\n",
    "    \n",
    "    # Show probability distribution\n",
    "    for i, p in enumerate(probs):\n",
    "        print(f\"  {label_mapping[i]}: {p:.4f} ({p*100:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Try Your Own Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your own text for classification\n",
    "your_text = input(\"Enter news text to classify: \")\n",
    "\n",
    "_, prediction, probs = predict_text(your_text)\n",
    "print(f\"\\nPredicted category: {prediction}\")\n",
    "\n",
    "# Visualize the probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(target_names, probs)\n",
    "plt.title('Category Probabilities')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Probability')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(probs):\n",
    "    plt.text(i, v + 0.01, f'{v:.2f}', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Loading the Model Later (Code Reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows how to load and use the saved model later\n",
    "'''\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your saved model\n",
    "peft_model_id = \"m3-peft-bert-base-uncased-agnews\"\n",
    "\n",
    "# Load the configuration\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "# Load the PEFT adapter weights\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "\n",
    "# Move to MPS device for M3\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Use for inference\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    label_mapping = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "    return label_mapping[predicted_class]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "documentqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
